{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Classification Empirical Study with Decision Trees**\n",
    "\n",
    "**Group Number:** 97  \n",
    "**Members:**  \n",
    "Roy Rui #300176548  \n",
    "Jiayi Ma #300263220\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset II: Zoo Animal Classification** \n",
    "Briefly introduce the background of the Zoo dataset, the goal of this notebook, and the main steps to be implemented.  \n",
    "- **Goal**: Use Decision Trees to classify animals into 7 categories (Mammal, Bird, Reptile, etc.).  \n",
    "- **Data Source**: Zoo Animal Classification - Kaggle / UCI Machine Learning Repository (Richard Forsyth, 1990).  \n",
    "- **Main Steps**: Data Cleaning, EDA and Outlier detection, Decision Trees, Feature Engineering, Empirical study, Result analysis.\n",
    "\n",
    "---\n",
    "\n",
    "# **Dataset Description**\n",
    "**Authors & Collaborators**: Richard Forsyth  \n",
    "**Source**: [Zoo Animal Classification](https://www.kaggle.com/datasets/uciml/zoo-animal-classification?select=zoo.csv)  \n",
    "**Shape**: **18 Columns, 101 Rows**  \n",
    "**Purpose**: Classify animals into seven categories based on their biological attributes, utilizing machine learning classification techniques.\n",
    "\n",
    "\n",
    "\n",
    "The Zoo dataset includes **101 animals** described by **16 traits** (mostly boolean) plus one numeric feature (`legs`). Each animal is assigned a label from **1 to 7**, corresponding to the category it belongs to. This dataset offers valuable insights into how specific physical and behavioral traits can be leveraged effectively in machine learning classification tasks. By leveraging these attributes, we aim to build a **Decision Tree** classifier that accurately predicts each animal’s category.  \n",
    "\n",
    "`zoo.csv` contains the core features and numeric labels.  \n",
    "`class.csv` provides a mapping from numeric labels to textual class descriptions, along with lists of animal names.  \n",
    "\n",
    "| Column       | Type    | Description                                              |\n",
    "|--------------|---------|----------------------------------------------------------|\n",
    "| `hair`       | Boolean | Whether the animal has hair (0 or 1)                    |\n",
    "| `feathers`   | Boolean | Whether the animal has feathers (0 or 1)                |\n",
    "| `eggs`       | Boolean | Whether the animal lays eggs (0 or 1)                   |\n",
    "| `milk`       | Boolean | Whether the animal produces milk (0 or 1)               |\n",
    "| `airborne`   | Boolean | Whether the animal can fly (0 or 1)                     |\n",
    "| `aquatic`    | Boolean | Whether the animal is aquatic (0 or 1)                  |\n",
    "| `predator`   | Boolean | Whether the animal is a predator (0 or 1)               |\n",
    "| `toothed`    | Boolean | Whether the animal has teeth (0 or 1)                   |\n",
    "| `backbone`   | Boolean | Whether the animal has a backbone (0 or 1)              |\n",
    "| `breathes`   | Boolean | Whether the animal breathes air (0 or 1)                |\n",
    "| `venomous`   | Boolean | Whether the animal is venomous (0 or 1)                 |\n",
    "| `fins`       | Boolean | Whether the animal has fins (0 or 1)                    |\n",
    "| `legs`       | Numeric | Number of legs (possible values: 0,2,4,5,6,8)            |\n",
    "| `tail`       | Boolean | Whether the animal has a tail (0 or 1)                  |\n",
    "| `domestic`   | Boolean | Whether the animal is domesticated (0 or 1)             |\n",
    "| `catsize`    | Boolean | Whether the animal is roughly cat-sized (0 or 1)        |\n",
    "| `class_type` | Numeric | Class label (1–7)                                        |\n",
    "\n",
    "\n",
    "The `class.csv` file accompanying this dataset serves as a reference, mapping numeric class labels to textual class descriptions and listing animal names within each class. This allows better interpretability of classification outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Import Necessary Libraries and Read Data**\n",
    "\n",
    "Here, we import the primary Python libraries needed for data analysis and machine learning:\n",
    "- **pandas**, **numpy**: for data manipulation and array operations\n",
    "- **sklearn.model_selection**: for data splitting and cross-validation\n",
    "- **sklearn.tree**: for the decision tree model\n",
    "- **sklearn.metrics**: for evaluation metrics (accuracy, f1_score, classification_report, confusion_matrix)\n",
    "- **sklearn.neighbors**: LocalOutlierFactor for outlier detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zoo.csv shape: (101, 18)\n",
      "  animal_name  hair  feathers  eggs  milk  airborne  aquatic  predator  \\\n",
      "0    aardvark     1         0     0     1         0        0         1   \n",
      "1    antelope     1         0     0     1         0        0         0   \n",
      "2        bass     0         0     1     0         0        1         1   \n",
      "3        bear     1         0     0     1         0        0         1   \n",
      "4        boar     1         0     0     1         0        0         1   \n",
      "\n",
      "   toothed  backbone  breathes  venomous  fins  legs  tail  domestic  catsize  \\\n",
      "0        1         1         1         0     0     4     0         0        1   \n",
      "1        1         1         1         0     0     4     1         0        1   \n",
      "2        1         1         0         0     1     0     1         0        0   \n",
      "3        1         1         1         0     0     4     0         0        1   \n",
      "4        1         1         1         0     0     4     1         0        1   \n",
      "\n",
      "   class_type  \n",
      "0           1  \n",
      "1           1  \n",
      "2           4  \n",
      "3           1  \n",
      "4           1  \n",
      "\n",
      "class.csv preview:\n",
      "   Class_Number  Number_Of_Animal_Species_In_Class Class_Type  \\\n",
      "0             1                                 41     Mammal   \n",
      "1             2                                 20       Bird   \n",
      "2             3                                  5    Reptile   \n",
      "3             4                                 13       Fish   \n",
      "4             5                                  4  Amphibian   \n",
      "\n",
      "                                        Animal_Names  \n",
      "0  aardvark, antelope, bear, boar, buffalo, calf,...  \n",
      "1  chicken, crow, dove, duck, flamingo, gull, haw...  \n",
      "2    pitviper, seasnake, slowworm, tortoise, tuatara  \n",
      "3  bass, carp, catfish, chub, dogfish, haddock, h...  \n",
      "4                             frog, frog, newt, toad  \n",
      "\n",
      "Merged view:\n",
      "  animal_name  class_type class_name\n",
      "0    aardvark           1     Mammal\n",
      "1    antelope           1     Mammal\n",
      "2        bass           4       Fish\n",
      "3        bear           1     Mammal\n",
      "4        boar           1     Mammal\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# Load zoo.csv and preview its shape and first few rows\n",
    "df_zoo = pd.read_csv(\"zoo.csv\")\n",
    "print(\"zoo.csv shape:\", df_zoo.shape)\n",
    "print(df_zoo.head())\n",
    "\n",
    "# Load class.csv for mapping numeric labels to descriptive class names and preview it\n",
    "df_class = pd.read_csv(\"class.csv\")\n",
    "print(\"\\nclass.csv preview:\")\n",
    "print(df_class.head())\n",
    "\n",
    "# Create a mapping from numeric class labels to text labels\n",
    "class_map = dict(zip(df_class[\"Class_Number\"], df_class[\"Class_Type\"]))\n",
    "\n",
    "# Add the textual class labels to df_zoo\n",
    "df_zoo[\"class_name\"] = df_zoo[\"class_type\"].map(class_map)\n",
    "\n",
    "# Display a merged view with animal_name, numeric and textual class labels\n",
    "print(\"\\nMerged view:\")\n",
    "print(df_zoo[[\"animal_name\", \"class_type\", \"class_name\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Data Cleaning**\n",
    "In this section, we handle any data cleaning steps in `zoo.csv`:\n",
    "- **Checked for missing values** using the `isnull().sum()` method.\n",
    "- **Confirmed no missing values existed**, so no imputation was required.\n",
    "- **Dropped irrelevant column (`animal_name`)** as it does not contribute to classification.\n",
    "\n",
    "We began by thoroughly inspecting the `zoo.csv` dataset for missing values. Since no missing values were detected, no further cleaning or imputation was necessary. Additionally, we removed the `animal_name` column to prevent irrelevant information from negatively influencing the classification.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of missing values in each column:\n",
      "animal_name    0\n",
      "hair           0\n",
      "feathers       0\n",
      "eggs           0\n",
      "milk           0\n",
      "airborne       0\n",
      "aquatic        0\n",
      "predator       0\n",
      "toothed        0\n",
      "backbone       0\n",
      "breathes       0\n",
      "venomous       0\n",
      "fins           0\n",
      "legs           0\n",
      "tail           0\n",
      "domestic       0\n",
      "catsize        0\n",
      "class_type     0\n",
      "class_name     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in each column of the dataframe\n",
    "missing_counts = df_zoo.isnull().sum()\n",
    "print(\"\\nNumber of missing values in each column:\")\n",
    "print(missing_counts)\n",
    "\n",
    "# Drop 'animal_name' as it's not useful for classification\n",
    "df_zoo.drop(columns=[\"animal_name\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Numerical feature encoding**\n",
    "\n",
    " \n",
    "Although decision trees can directly handle numerical features, **binning** can sometimes improve interpretability or performance by segmenting a continuous (or discrete) feature into categories. In this example, we apply a simple binning strategy to the `legs` column:\n",
    "\n",
    "- **0:** Animals with 2 or fewer legs  \n",
    "- **1:** Animals with more than 2 but fewer than 6 legs  \n",
    "- **2:** Animals with 6 or more legs  \n",
    "\n",
    "This creates a new feature `legs_binned`, which may be used alongside or instead of the original `legs` column during classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric feature binning and outlier detection with LOF\n",
    "def bin_legs(x):\n",
    "    if x <= 2:\n",
    "        return 0\n",
    "    elif 2 < x < 6:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "# Apply binning to 'legs' and create 'legs_binned'\n",
    "df_zoo[\"legs_binned\"] = df_zoo[\"legs\"].apply(bin_legs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Outlier Detection**\n",
    "\n",
    "- **Applied Local Outlier Factor (LOF)** after converting boolean columns into numerical format (0/1).\n",
    "- **Identified outliers** based on a contamination parameter set at 5%.\n",
    "- **Removed the identified outliers** to produce a cleaned dataset (`df_no_outlier`) for further analysis.\n",
    "\n",
    "To ensure dataset quality, we employed the LOF method to detect and remove outliers. After identifying a few data points marked as outliers, we created a cleaned dataset (`df_no_outlier`). This section enabled us to later compare the impact of outlier removal on model accuracy.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LOF results:\n",
      "outlier\n",
      " 1    96\n",
      "-1     5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Prepare numeric data by excluding non-numeric target columns\n",
    "numeric_cols = df_zoo.drop(columns=[\"class_type\", \"class_name\"]).columns\n",
    "df_zoo_numeric = df_zoo[numeric_cols].copy()\n",
    "\n",
    "# Convert boolean columns to integers\n",
    "for c in df_zoo_numeric.columns:\n",
    "    if df_zoo_numeric[c].dtype == bool:\n",
    "        df_zoo_numeric[c] = df_zoo_numeric[c].astype(int)\n",
    "\n",
    "# Run LOF to detect outliers with 5% contamination\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\n",
    "outlier_labels = lof.fit_predict(df_zoo_numeric)\n",
    "df_zoo[\"outlier\"] = outlier_labels\n",
    "\n",
    "# Display outlier counts\n",
    "print(\"\\nLOF results:\")\n",
    "print(df_zoo[\"outlier\"].value_counts())\n",
    "\n",
    "# Remove outliers to create a cleaned dataset\n",
    "df_no_outlier = df_zoo[df_zoo[\"outlier\"] != -1].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Predictive analysis: Decision Trees**\n",
    "\n",
    "- Explore the `DecisionTreeClassifier` method suggested in scikit-learn (or other packages).  \n",
    "- Look at the parameters (e.g., splitting criterion like gini or entropy, max_depth, min_samples_split, etc.) and choose a baseline setting.\n",
    "\n",
    "In this section, we define a helper function `prepare_features()` that **extracts only the relevant feature columns** (`class_type`, `class_name`, `outlier`). This helps streamline our process of training and comparing different decision tree configurations. \n",
    "\n",
    "We will later instantiate a `DecisionTreeClassifier` to train it on these prepared features, and evaluate it as per the assignment steps. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(dataframe):\n",
    "    # Exclude class_type, class_name, and outlier from features\n",
    "    return dataframe.drop(columns=[\"class_type\", \"class_name\", \"outlier\"], errors=\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Feature Engineering**\n",
    "\n",
    "- **Created two new features** to fulfill assignment criteria:\n",
    "  - **`milk_hair`**: indicating animals that produce milk and have hair.\n",
    "  - **`milk_feathers`**: indicating animals that produce milk and have feathers (used to examine unusual feature combinations).\n",
    "\n",
    "In this section, we introduced two new engineered features (`milk_hair` and `milk_feathers`) designed to evaluate possible interactions between biological attributes. This step potentially provided the decision tree with enhanced discriminative capabilities, improving classification performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st feature: milk_hair\n",
    "df_zoo[\"milk_hair\"] = (df_zoo[\"milk\"] & df_zoo[\"hair\"]).astype(int)\n",
    "df_no_outlier[\"milk_hair\"] = (df_no_outlier[\"milk\"] & df_no_outlier[\"hair\"]).astype(int)\n",
    "\n",
    "# 2nd feature: milk_feathers\n",
    "df_zoo[\"milk_feathers\"] = (df_zoo[\"milk\"] & df_zoo[\"feathers\"]).astype(int)\n",
    "df_no_outlier[\"milk_feathers\"] = (df_no_outlier[\"milk\"] & df_no_outlier[\"feathers\"]).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Empirical Study**\n",
    "\n",
    "- **Split the dataset** into train-validation (80%) and test set (20%).\n",
    "- **Evaluated three configurations** through 4-fold cross-validation:\n",
    "  - **Baseline**: Original data, no outlier removal, no feature engineering.\n",
    "  - **Feature Engineering (FeatEng)**: Included two new engineered features, no outlier removal.\n",
    "  - **NoOutlier + FeatEng**: Combined outlier removal and engineered features.\n",
    "- **Compared configurations using accuracy metrics**.\n",
    "\n",
    "The dataset was systematically split, and three configurations were rigorously compared using 4-fold cross-validation on the training-validation subset. Accuracy scores obtained from these experiments were clearly documented to facilitate the selection of the best configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========= Cross-validation results (4-Fold) =========\n",
      "Baseline (no new features, no outlier removal): 0.9308\n",
      "FeatEng (with new features, no outlier removal): 0.9308\n",
      "NoOutlier+FeatEng (with outlier removal and new features): 0.9062\n"
     ]
    }
   ],
   "source": [
    "# Split full dataset into train-validation and test sets (80:20)\n",
    "X_all = prepare_features(df_zoo)\n",
    "y_all = df_zoo[\"class_type\"]\n",
    "\n",
    "X_trainVal, X_test, y_trainVal, y_test = train_test_split(\n",
    "    X_all, y_all, test_size=0.2, random_state=42, stratify=y_all\n",
    ")\n",
    "\n",
    "# Split outlier-removed dataset into train-validation and test sets\n",
    "X_all_no_outlier = prepare_features(df_no_outlier)\n",
    "y_all_no_outlier = df_no_outlier[\"class_type\"]\n",
    "\n",
    "X_trainVal_no_out, X_test_no_out, y_trainVal_no_out, y_test_no_out = train_test_split(\n",
    "    X_all_no_outlier, y_all_no_outlier, test_size=0.2, random_state=42, stratify=y_all_no_outlier\n",
    ")\n",
    "\n",
    "# Create baseline datasets by dropping engineered features\n",
    "df_baseline = df_zoo.drop(columns=[\"milk_hair\", \"milk_feathers\"], errors=\"ignore\").copy()\n",
    "df_baseline_no_outlier = df_no_outlier.drop(columns=[\"milk_hair\", \"milk_feathers\"], errors=\"ignore\").copy()\n",
    "\n",
    "# Define a function for 4-fold cross-validation to compute average accuracy\n",
    "def cross_val_accuracy(X, y, n_splits=4):\n",
    "    model = DecisionTreeClassifier(random_state=42)\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n",
    "    return scores.mean()\n",
    "\n",
    "# Evaluate Baseline: no new features, no outlier removal\n",
    "X_bsl = df_baseline.drop(columns=[\"class_type\", \"class_name\", \"outlier\"], errors=\"ignore\")\n",
    "y_bsl = df_baseline[\"class_type\"]\n",
    "acc_baseline = cross_val_accuracy(X_bsl, y_bsl, 4)\n",
    "\n",
    "# Evaluate with Feature Engineering (new features, no outlier removal)\n",
    "X_feat = prepare_features(df_zoo)\n",
    "y_feat = df_zoo[\"class_type\"]\n",
    "acc_feat = cross_val_accuracy(X_feat, y_feat, 4)\n",
    "\n",
    "# Evaluate with Outlier Removal + Feature Engineering (new features, outliers removed)\n",
    "X_noOut_feat = prepare_features(df_no_outlier)\n",
    "y_noOut_feat = df_no_outlier[\"class_type\"]\n",
    "acc_noOut_feat = cross_val_accuracy(X_noOut_feat, y_noOut_feat, 4)\n",
    "\n",
    "print(\"\\n========= Cross-validation results (4-Fold) =========\")\n",
    "print(f\"Baseline (no new features, no outlier removal): {acc_baseline:.4f}\")\n",
    "print(f\"FeatEng (with new features, no outlier removal): {acc_feat:.4f}\")\n",
    "print(f\"NoOutlier+FeatEng (with outlier removal and new features): {acc_noOut_feat:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Result Analysis**\n",
    "\n",
    "- **Selected the best-performing configuration (NoOutlier + FeatEng)** based on cross-validation results.\n",
    "- **Trained final model** on the full training-validation set using the chosen configuration.\n",
    "- **Evaluated performance** on the previously unseen test set, reporting metrics such as Accuracy, F1-score, classification report, and confusion matrix.\n",
    "- **Discussed and compared final results** with cross-validation performance to assess consistency and generalizability.\n",
    "\n",
    "After general evaluation, the optimal configuration was selected for final assessment. This model was then trained on the entire training-validation dataset and evaluated on the test set. The resulting metrics, along with a comparison to cross-validation outcomes, provided insights into the stability and predictive strength of the classification model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========= Final Test Set Results =========\n",
      "Test Accuracy: 1.0000\n",
      "Test F1 (macro): 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         8\n",
      "           2       1.00      1.00      1.00         4\n",
      "           3       1.00      1.00      1.00         1\n",
      "           4       1.00      1.00      1.00         3\n",
      "           6       1.00      1.00      1.00         2\n",
      "           7       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00        20\n",
      "   macro avg       1.00      1.00      1.00        20\n",
      "weighted avg       1.00      1.00      1.00        20\n",
      "\n",
      "Confusion Matrix:\n",
      "[[8 0 0 0 0 0]\n",
      " [0 4 0 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 0 3 0 0]\n",
      " [0 0 0 0 2 0]\n",
      " [0 0 0 0 0 2]]\n"
     ]
    }
   ],
   "source": [
    "# Train and predict\n",
    "dtc_best = DecisionTreeClassifier(random_state=42)\n",
    "dtc_best.fit(X_trainVal_no_out, y_trainVal_no_out)\n",
    "y_pred_test = dtc_best.predict(X_test_no_out)\n",
    "\n",
    "# Compute metrics\n",
    "acc_test = accuracy_score(y_test_no_out, y_pred_test)\n",
    "f1_test = f1_score(y_test_no_out, y_pred_test, average='macro')\n",
    "\n",
    "print(\"\\n========= Final Test Set Results =========\")\n",
    "print(f\"Test Accuracy: {acc_test:.4f}\")\n",
    "print(f\"Test F1 (macro): {f1_test:.4f}\")\n",
    "\n",
    "# Show detailed report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_no_out, y_pred_test))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_no_out, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Conclusion**\n",
    "\n",
    "In this classification task, we successfully implemented data cleaning, outlier detection (LOF), and created two new features (`milk_hair` and `milk_feathers`). We compared three different configurations (Baseline, Feature Engineering, and Outlier Removal + Feature Engineering) using a 4-fold cross-validation setup. Although removing outliers slightly reduced cross-validation accuracy, the final model evaluated on the test set achieved perfect accuracy (100%), indicating strong performance.  \n",
    "\n",
    "Future improvements may include experimenting with ensemble methods or optimizing decision tree parameters for potential performance gains.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **References**\n",
    "1. **Zoo Animal Classification**: [Kaggle - Zoo Animal Classification](https://www.kaggle.com/datasets/uciml/zoo-animal-classification?select=zoo.csv)  \n",
    "2. **Assignment 3 PDF**: *CSI4142 Data Science, CSI4142-Assignment3-Description*  \n",
    "3. **PredictiveAnalysis-DecisionTrees**: *CSI4142 Data Science, Winter2025-CSI4142-Week7-PredictiveAnalysis-DecisionTrees*   \n",
    "4. **Python Outlier Detection with Local Outlier Factor (LOF)**: [YouTube Video](https://www.youtube.com/watch?v=_LEaSHhcNGw)\n",
    "5. **Decision Tree Classification in Python (from scratch!)**: [YouTube Video](https://www.youtube.com/watch?v=sgQAhG5Q7iY)\n",
    "\n",
    "---\n",
    "\n",
    "## **Acknowledgments**\n",
    "- **ChatGPT**: Formatting markdown texts, paraphrasing, grammar checks, and code debugging.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
